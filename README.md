# End-to-End Big Data Pipeline

A complete, production-ready data engineering project demonstrating a modern data pipeline architecture using containerized services.

## ğŸ“Š Project Overview

This project implements a comprehensive data pipeline that ingests real-time data, processes it at scale, stores it in multiple databases, and visualizes insights through interactive dashboards.

**Data Flow:**
```
Random User API â†’ Kafka (Message Queue) â†’ Spark (Processing) â†’ PostgreSQL/Cassandra (Storage) â†’ Superset (Visualization)
```

## ğŸ—ï¸ Architecture

### Components

| Component | Purpose | Technology |
|-----------|---------|-----------|
| **Data Ingestion** | Real-time data streaming | Kafka, Python |
| **Orchestration** | Workflow automation & scheduling | Apache Airflow |
| **Stream Processing** | Real-time data transformation | Apache Spark |
| **Data Storage** | Persistent data storage | PostgreSQL, Cassandra |
| **Visualization** | Business Intelligence dashboards | Apache Superset |
| **Containerization** | Environment standardization | Docker & Docker Compose |

### Service Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    External API                         â”‚
â”‚              (RandomUser.me)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Airflow DAG                            â”‚
â”‚          (Workflow Orchestration)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Kafka        â”‚    â”‚     Spark        â”‚
â”‚  (Message Bus)   â”‚â”€â”€â”€â†’â”‚  (Processing)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“                                   â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ PostgreSQL  â”‚               â”‚   Cassandra    â”‚
    â”‚  (OLTP)     â”‚               â”‚   (NoSQL)      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“                                   â†“
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚   Superset    â”‚
                  â”‚  (Dashboards) â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- Docker & Docker Compose
- Python 3.10+
- Git

### Installation

1. **Clone the repository**
```bash
git clone <repository-url>
cd E2E
```

2. **Start all services**
```bash
docker compose up -d
```

3. **Verify services are running**
```bash
docker compose ps
```

4. **Access the services**
- Airflow UI: http://localhost:8080
- Superset UI: http://localhost:8088
- Kafka: localhost:9092
- PostgreSQL: localhost:5432
- Cassandra: localhost:9042

### Default Credentials

| Service | Username | Password |
|---------|----------|----------|
| Airflow | airflow | airflow |
| Superset | admin | admin |
| PostgreSQL (Airflow) | airflow | airflow |
| PostgreSQL (Superset) | superset | superset |
| Cassandra | â€” | â€” |

## ğŸ“ Project Structure

```
E2E/
â”œâ”€â”€ airflow/                      # Airflow orchestration
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ kafka_stream.py      # Main streaming DAG
â”‚   â”œâ”€â”€ docker-compose.yml        # Airflow services
â”‚   â”œâ”€â”€ Dockerfile               # Custom Airflow image
â”‚   â”œâ”€â”€ requirements.txt          # Python dependencies
â”‚   â””â”€â”€ config/
â”‚       â””â”€â”€ airflow.cfg          # Airflow configuration
â”œâ”€â”€ kafka/                        # Kafka setup
â”‚   â””â”€â”€ docker-compose.yml        # Kafka & Zookeeper services
â”œâ”€â”€ spark/                        # Spark processing
â”‚   â””â”€â”€ docker-compose.yml        # Spark services
â”œâ”€â”€ postgres/                     # PostgreSQL database
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ airflow_init.sql         # Airflow DB initialization
â”‚   â””â”€â”€ superset_init.sql        # Superset DB initialization
â”œâ”€â”€ cassandra/                    # Cassandra NoSQL database
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ superset/                     # Superset BI platform
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â””â”€â”€ docker/
â”‚       â”œâ”€â”€ superset_config.py    # Superset configuration
â”‚       â”œâ”€â”€ docker-bootstrap.sh   # Bootstrap script
â”‚       â””â”€â”€ .env                  # Environment variables
â”œâ”€â”€ docker-compose.yml            # Root compose (includes all services)
â””â”€â”€ scripts/                      # Utility scripts
    â””â”€â”€ entrypoint.sh            # Service initialization
```

## ğŸ”„ Data Pipeline Details

### 1. Data Ingestion (Kafka Stream)
The DAG in [airflow/dags/kafka_stream.py](airflow/dags/kafka_stream.py) performs:
- Fetches random user data from RandomUser.me API
- Formats and structures the data
- Produces messages to Kafka topic
- Scheduled to run periodically

**Topics:**
- `users_topic` - Raw user data stream

### 2. Stream Processing (Spark)
Spark jobs consume from Kafka and:
- Apply transformations and data quality checks
- Enrich data with additional attributes
- Write processed data to storage layer

### 3. Data Storage
**PostgreSQL** (Relational):
- Structured user profiles
- Metadata and configurations
- Optimized for OLTP queries

**Cassandra** (NoSQL):
- Time-series user events
- High-write throughput
- Distributed storage for scalability

### 4. Data Visualization (Superset)
Interactive dashboards displaying:
- User demographics
- Activity patterns
- Real-time metrics

## ğŸ› ï¸ Configuration

### Environment Variables

Key environment files:
- [postgres/.env](postgres/.env) - PostgreSQL configuration
- [superset/docker/.env](superset/docker/.env) - Superset configuration

**Modify these to customize:**
- Database credentials
- Service ports
- Log levels
- Performance tuning parameters

### Airflow Configuration
Edit [airflow/config/airflow.cfg](airflow/config/airflow.cfg) for:
- Parallelism settings
- Executor configuration
- DAG parsing behavior

## ğŸ“Š Database Schemas

### PostgreSQL - User Profile Table
```sql
CREATE TABLE users (
    user_id SERIAL PRIMARY KEY,
    first_name VARCHAR(255),
    last_name VARCHAR(255),
    email VARCHAR(255) UNIQUE,
    phone VARCHAR(20),
    nationality VARCHAR(5),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### Cassandra - User Events Table
```cql
CREATE TABLE user_events (
    event_id UUID PRIMARY KEY,
    user_id INT,
    event_type TEXT,
    event_data MAP<TEXT, TEXT>,
    timestamp BIGINT,
    created_at TIMESTAMP
);
```

## ğŸ” Monitoring & Debugging

### View Logs
```bash
# Airflow logs
docker logs airflow-scheduler

# Spark logs
docker logs spark-master

# Kafka logs
docker logs broker

# PostgreSQL logs
docker logs postgres
```

### Health Checks
```bash
# Check all services
docker compose ps

# Check Kafka broker
docker exec broker kafka-broker-api-versions.sh --bootstrap-server localhost:9092

# Check PostgreSQL connection
psql -h localhost -U airflow -d airflow
```

### Common Issues

**Database Connection Error**
```bash
# Verify network connectivity
docker network inspect e2e_airflow-network

# Restart database
docker compose -f postgres/docker-compose.yml restart
```

**Kafka Producer Issues**
```bash
# Check Kafka broker status
docker logs broker | grep ERROR

# Verify topic exists
docker exec broker kafka-topics.sh --list --bootstrap-server localhost:9092
```

## ğŸš¢ Deployment

### Production Considerations

1. **Security**
   - Use environment-specific .env files
   - Enable SSL/TLS for database connections
   - Implement authentication for all services

2. **Scaling**
   - Increase Spark executor count
   - Configure Cassandra replication factor
   - Set up Kafka partitioning strategy

3. **Monitoring**
   - Implement centralized logging (ELK stack)
   - Add metrics collection (Prometheus)
   - Set up alerting mechanisms

4. **Backup & Recovery**
   - PostgreSQL backup strategy
   - Cassandra snapshot management
   - Kafka topic retention policies

## ğŸ“š Key Technologies

- **Apache Airflow** - Workflow orchestration
- **Apache Kafka** - Event streaming platform
- **Apache Spark** - Distributed data processing
- **PostgreSQL** - Relational database
- **Apache Cassandra** - NoSQL database
- **Apache Superset** - Data visualization
- **Docker & Docker Compose** - Containerization

## ğŸ¤ Contributing

1. Create a feature branch (`git checkout -b feature/improvement`)
2. Make your changes
3. Commit with descriptive messages
4. Push to the branch
5. Open a Pull Request

## ğŸ“ License

This project is open source and available under the MIT License.

## ğŸ“§ Support

For issues, questions, or suggestions:
- Open an issue on GitHub
- Create a discussion for feature requests

---

**Built with â¤ï¸ for learning and production-ready data engineering**
